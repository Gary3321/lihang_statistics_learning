决策树优点：模型具有可读性，分类速度快。

学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树
模型进行分类。

决策树学习是右训练数据集估计条件概率模型。

决策树学习的损失函数通常是正则化的极大似然函数； 决策树学习的策略是以损失函数为目标函数的最小化。

决策树的学习通常包括3个步骤：
- 特征选择
- 决策树的生成
- 决策树的修剪

决策树是一种树形结构，由**节点**和**有向边**组成。节点有两种类型：内部节点和叶节点。内部节点
表示一个特征或属性，叶节点表示一个类。

决策树学习的算法通常是一个递归地选择最优特征。

决策树的构建：
- 构建根节点，将所有训练数据都放在跟节点。
- 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。
- 如果这些子集已经能够被基本正确分类，那么构建叶节点。否则，继续选择最优特征进去数据集的分割。

为了避免过拟合，我们需要对已生成的树**自下而上**进行剪枝，将树变的更简单，从而使它具有更好的泛化能力。

### 5.1 特征选择
特征选择的准则是**信息增益**或**信息增益比**

熵(entropy)表示随机变量不确定性的度量。

$H(p)=-\sum_{i=1}^n p_i \log p_i$ (5.2)

$P(X=x_i)=p_i$, i = 1, 2, ..., n

熵越大，随机变量的不确定性就越大； 均匀分布时熵最大。

条件熵 H(Y|X) 表示在已知随机变量X的条件下随机变量Y的不确定性。

$H(Y|X)=\sum_{i=1}^n p_i H(Y|X=x_i)$, 这里
$p_i = P(X=x_i)$

**信息增益(information gain)** 表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。
![定义](/pictures/information-gain-def.png)

决策树学习应用信息增益准则选择特征；信息增益大的特征具有更强的分类能力。 具体方法：对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用 **信息增益比(information gain ratio)** 可以对这一问题进行校正。
![定义](/pictures/information-gain-ratio.png)
