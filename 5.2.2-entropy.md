## 熵(entropy)
在信息论与概率统计中, **熵(entropy)** 是表示随机变量**不确定性**的度量。
设X是一个取有限个值的离散随机变量，其概率分布为:

$P(X=x_i)=p_i$, i=1,2,...,n （X取值$x_i$的概率是$p_i$）

则随机变量x的熵定义为：

$H(X)=-\sum_{i=1}^n p_i \log p_i$ (5.1)

在式5.1中，若$p_i=0$,则定义$0\log 0 =0$。 通常 log 是以2或e为底，这时熵的单位分别称作比特（bit）或纳特（nat).

由定义可知，熵只依赖于**X的分布**， 而于X的取值无关，所以熵也可以记作H(p),即：

$H(p)=-\sum_{i=1}^n p_i \log p_i$

**熵越大随机变量的不确定性就越大** （当概率为0.5时，不确定性最大）。

从定义可验证 $0\le H(p) \le \log n$. (**如何验证？**)

## 条件熵
设有随机变量（X,Y)，其联合概率分布为：

$P(X=x_i, Y=y_j)=p_{ij}$, i=1,2,...,n; j=1,2,...,m

条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，定义为 X给定条件下Y的条件概率分布的熵对X的数学期望：

$H(Y|X)=\sum_{i=1}^n p_i H(Y|X=x_i)$, $p_i = P(X=x_i)$, i=1,2,...,n.

当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分布称为经验熵（enpirical entropy） 和经验条件熵（empirical conditional entropy）。
