### 问题：K-NN 严重依赖于测试集，什么情况下使用它呢？

k 近邻法（k-nearest neighbor, k-NN）是一种基本的分类与回归方法。

k-NN假设给定一个训练数据集；k-NN不具有显示的学习过程； 它利用训练数据集对特征向量空间进行划分，并作为其分类的模型。 （意味着：如果测试数据和训练数据区别大，这个算法表现不好？？）
## 3.1 k近邻算法
k-NN算法简单、直观：给定一个训练集，对新的输入实例，在**训练集**中找到与这个实例**最邻近的k个实例**，这个k个实例的多数属于某个类，就把该输入实例分为这个类。因此，K-NN没有显示的学习过程。
![k-NN algorithm](/pictures/k-NN.png)

## 3.2 k近邻模型
k-NN模型有三个基本要素 - 距离度量、k值的选择和分类决策规则决定。
- 距离度量： 一般采用$L_p$距离。 由不同的距离度量所确定的最近邻点是不同的。
![Lp-distance](/pictures/Lp-distance.png)
- k值的选择：k值的选择会对k-NN的结果产生重大影响。k值的减少意味中模型变得复杂，容易发生过拟合；k值的增大意味着模型变的简单 （如果k=N,无论输入实例是什么，都简单预测为训练集中最多的类）。在应用中，通常采用交叉验证法来选取最优的k值。
- 分类决策规则：往往是多数表决，即由输入实例的k个邻近的训练集中的**多数类**决定输入实例的类。

## 3.3 k-NN的实现：kd树
如何对训练数据进行快速的k近邻搜索？ -- kd树
- 构造kd树
  - 二叉树；对k维空间的实例点进行存储以便对其快速检索的树形结构。
